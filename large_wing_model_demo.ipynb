{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "import os\n",
    "import tqdm.notebook as tn \n",
    "import torch \n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations\n",
    "### GPU Usage\n",
    "Determine if you want to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE] Planning to run on GPU 0\n"
     ]
    }
   ],
   "source": [
    "def print_with_tag(message, tag=None):\n",
    "    return print(f\"[{tag}] {message}\" if tag else message)\n",
    "\n",
    "def initialize_devices(use_gpu, gpu_id=None):\n",
    "    print_tag = 'DEVICE'\n",
    "    if use_gpu == True: # USE GPU\n",
    "        if gpu_id is not None: # user has specfied a GPU number\n",
    "            # Make sure the gpu_id is an integer \n",
    "            assert isinstance(gpu_id, int), f\"Invalid GPU ID: {gpu_id}. GPU ID must be an integer.\"\n",
    "        else:\n",
    "            gpu_id = 0  # default GPU id if not specified\n",
    "             \n",
    "        output_device = torch.device('cuda:'+str(gpu_id)) # output device \n",
    "        print_with_tag(f\"Planning to run on GPU {gpu_id}\", print_tag)\n",
    "         \n",
    "    elif use_gpu == False: # DO NOT USE GPU\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # make sure no GPUs are available in the environment\n",
    "        output_device = torch.device('cpu')\n",
    "        print_with_tag('Using CPU.', print_tag)\n",
    "        \n",
    "    else: # INVALID INPUT, USE CPU\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" # make sure no GPUs are available in the environment\n",
    "        output_device = torch.device('cpu')\n",
    "        print_with_tag('Invalid use_gpu parameter. Defaulting to CPU.', print_tag)\n",
    "    return output_device \n",
    "\n",
    "output_device = initialize_devices(use_gpu=True, gpu_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the following from the CSV = ['xc', 'yb', 'surf', 'airfoil', 'alpha', 'M', 'Re', 'chord',' taper ratio', 'span', 'le sweep', 'cp', 'source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_set(cp_file_path, coordinates_file_path, test_case_nums):\n",
    "    # load in CSV file \n",
    "    raw_data = pd.read_csv(cp_file_path)\n",
    "    raw_afcoord_data = pd.read_csv(coordinates_file_path).values[:, 1:]\n",
    "    \n",
    "    # Pre-proess training data \n",
    "    ## subsample to below stall angles \n",
    "    data = raw_data[raw_data['alpha']<=12]\n",
    "    af_coord = raw_afcoord_data[raw_data['alpha']<=12] \n",
    "    data = data[['xc','surf','yb', 'alpha', 'M', 'chord', 'taper ratio', 'span', 'le sweep', 'cp', 'case', 'source','airfoil']]\n",
    "\n",
    "    ## Convert xc to xhat and yhat\n",
    "    xhat = data['xc'].values*2-1\n",
    "    yhat = np.where(data['surf'] == 'U', np.sin(np.arccos(xhat)), -np.sin(np.arccos(xhat)))\n",
    "\n",
    "    ## Convert degrees to radians\n",
    "    alpha = torch.deg2rad(torch.tensor(data['alpha'].values))\n",
    "    sweep_alpha = torch.deg2rad(torch.tensor(data['le sweep'].values))\n",
    "\n",
    "    ## Non-dimensionalize span \n",
    "    spanwise = data['yb'].values#*data['span'].values/data['chord'].values\n",
    "    ar = (data['span'].values/data['chord'].values)\n",
    "\n",
    "    ## Select test cases \n",
    "    test_cases_all = np.array(test_case_nums)\n",
    "    test_indices = np.where(np.isin(data['case'].values.flatten(), test_cases_all))[0]\n",
    "\n",
    "    ## Select train cases \n",
    "    train_target_cases = np.arange(1, np.max(data['case'].values)+1)\n",
    "    train_target_cases = np.delete(train_target_cases, test_cases_all-1)\n",
    "    train_indices = np.nonzero(np.isin(data['case'], train_target_cases))[0]\n",
    "    \n",
    "    ## All cases\n",
    "    all_x = torch.tensor(np.hstack((af_coord, xhat.reshape((-1,1)), yhat.reshape((-1,1)), spanwise.reshape((-1,1)), alpha.reshape((-1,1)), data[['M', 'taper ratio']].values, sweep_alpha.reshape((-1,1)), ar.reshape((-1,1)), data[['case']].values) ) )\n",
    "    all_cases = data[['case']].values.flatten()\n",
    "    all_y = torch.tensor(data['cp'].values) \n",
    "\n",
    "    ## Training cases \n",
    "    train_x = torch.tensor(np.hstack((af_coord, xhat.reshape((-1,1)), yhat.reshape((-1,1)), spanwise.reshape((-1,1)), alpha.reshape((-1,1)), data[['M', 'taper ratio']].values, sweep_alpha.reshape((-1,1)), ar.reshape((-1,1)), data[['case']].values) ) )[train_indices]\n",
    "    train_cases = data[['case']].values[train_indices].flatten()\n",
    "    train_y = torch.tensor(data['cp'].values)[train_indices]\n",
    "    scaler_mean = torch.mean(train_y)\n",
    "    scaler_scale = 10\n",
    "    train_y = (train_y - scaler_mean) * scaler_scale\n",
    "    train_afs = af_coord[train_indices]\n",
    "\n",
    "    ## Test cases \n",
    "    test_x = torch.tensor(np.hstack((af_coord, xhat.reshape((-1,1)), yhat.reshape((-1,1)), spanwise.reshape((-1,1)), alpha.reshape((-1,1)), data[['M', 'taper ratio']].values, sweep_alpha.reshape((-1,1)), ar.reshape((-1,1)), data[['case']].values) ) )[test_indices]\n",
    "    test_y = torch.tensor(data['cp'].values)[test_indices]\n",
    "    test_y = (test_y - scaler_mean) * scaler_scale\n",
    "    test_cases = data[['case']].values[test_indices].flatten()\n",
    "    test_afs = af_coord[test_indices]\n",
    "    test_std = raw_data[raw_data['alpha']<=12.0]['std'].values[test_indices]\n",
    "    test_surf = data[['surf']].values[test_indices].flatten()\n",
    "\n",
    "    return [all_x, all_y, all_cases], [train_x, train_y, train_cases, train_afs, train_indices], [test_x, test_y, test_cases, test_std, test_afs, test_surf], [data, af_coord]\n",
    "\n",
    "    \n",
    "alls, trains, tests, [raw_data, raw_af_coord] = create_train_test_set(\n",
    "    cp_file_path = './data/wing_dataset_20241114_moredata.csv', \n",
    "    coordinates_file_path = './data/airfoil_coordinates_20241114_moredata.csv',\n",
    "    test_case_nums=[44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
    "       61, 62, 63, 92, 93, 94, 95, 96, 97, 98, 100, 91, 14, 64, 65, 66, 67, 68]\n",
    ")\n",
    "\n",
    "all_x = alls[0]\n",
    "train_x, train_y, train_case, train_af, train_inds = trains[0], trains[1], trains[2], trains[3], trains[4]\n",
    "test_x, test_y, test_case, test_std, test_afs, test_surf = tests[0], tests[1], tests[2], tests[3], tests[4], tests[5]\n",
    "all_cases_unique = np.unique(alls[2])\n",
    "\n",
    "# push to gpu\n",
    "train_x = train_x.to(output_device)\n",
    "train_y = train_y.to(output_device)\n",
    "scaler_mean = -0.2241\n",
    "scaler_scale = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- training data goes from 1.0 (TE, upper) to 0.0 to -1.0 (TE, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/wing_dataset_20241114_moredata.csv')\n",
    "\n",
    "## subsample to below stall angles \n",
    "data = raw_data[raw_data['alpha']<=12] \n",
    "data = data[['xc','surf','yb', 'alpha', 'M', 'chord', 'taper ratio', 'span', 'le sweep', 'cp', 'case', 'source','airfoil']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select specific test case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Test Case Information: \n",
      "    Source: NASA-TP-3151\n",
      "    Geometry: \n",
      "        Airfoil section: NACA0015\n",
      "        LE sweep angle : 0.0 deg\n",
      "        Taper ratio    : 1.0\n",
      "        Semi-span / c  : 3.29\n",
      "    Operating condition: \n",
      "        Angle of attack : 4.1 deg\n",
      "        Freestream Mach : 0.17\n"
     ]
    }
   ],
   "source": [
    "# Best used for validation data \n",
    "def select_test_case(case_num, tests):\n",
    "    # select a single case from test data\n",
    "    test_x_, test_y_, test_case_, test_std_, test_afs_, test_surf_ = tests[0], tests[1], tests[2], tests[3], tests[4], tests[5]\n",
    "    sel_indices = np.nonzero(np.isin(test_case_, case_num))[0]\n",
    "    true_test_x = test_x_[sel_indices]\n",
    "    true_test_y = test_y_[sel_indices]/scaler_scale + scaler_mean\n",
    "    true_test_std = test_std_[sel_indices]\n",
    "    true_test_surf = test_surf_[sel_indices] \n",
    "        \n",
    "    # Override test case 3 with tunnel corrections \n",
    "    # Note that other cases in this technical expt were not corrected\n",
    "    if case_num == [92]: \n",
    "        testcase3 = pd.read_csv('./data/testcase3_tunnel_corrections.csv')\n",
    "        af_coords_ = np.tile(test_afs_[sel_indices][0], (testcase3['xc'].shape[0], 1))\n",
    "        xhat_ = (testcase3['xc'].values*2-1)[:,None]\n",
    "        yhat_ = np.where(testcase3['surf'] == 'U', np.sin(np.arccos(xhat_.flatten())), -np.sin(np.arccos(xhat_.flatten())))[:,None]\n",
    "        alpha_ = np.deg2rad(4.1)*np.ones_like(xhat_) # 0.0698\n",
    "        sweep_alpha_ = 0.0*np.ones_like(xhat_)\n",
    "        spanwise_ = testcase3['yb'].values[:,None]\n",
    "        ar_ = 3.2882*np.ones_like(xhat_)  \n",
    "        m_ = 0.17*np.ones_like(xhat_)\n",
    "        tr_ = 1.0*np.ones_like(xhat_) \n",
    "        true_test_x = torch.tensor(np.hstack((af_coords_, xhat_, yhat_, spanwise_, alpha_, m_, tr_, sweep_alpha_, ar_, np.ones_like(xhat_)*92)))\n",
    "        true_test_y = torch.tensor(testcase3['cp'].values) \n",
    "        true_test_std = torch.zeros_like(torch.tensor(xhat_))\n",
    "        true_test_surf = testcase3['surf'].values\n",
    "        \n",
    "    print_with_tag('Test Case Information: ', 'LOG')\n",
    "    print('    Source: '  + raw_data['source'][raw_data['case']==case_num[0]].values[0])\n",
    "    print('    Geometry: ')\n",
    "    print('        Airfoil section: ' + raw_data['airfoil'][raw_data['case']==case_num[0]].values[0])\n",
    "    print('        LE sweep angle : ' + str(np.round(np.rad2deg(true_test_x[0, -3].item()), 2)) + ' deg')\n",
    "    print('        Taper ratio    : ' + str(true_test_x[0, -4].item()))\n",
    "    print('        Semi-span / c  : ' + str(np.round(true_test_x[0, -2].item(), 2)))\n",
    "    print('    Operating condition: ')\n",
    "    print('        Angle of attack : ' + str(np.round(np.rad2deg(true_test_x[0, -6].item()), 2)) + ' deg')\n",
    "    print('        Freestream Mach : ' + str(true_test_x[0, -5].item()))\n",
    "    return [true_test_x, true_test_y, true_test_std, true_test_surf] \n",
    "\n",
    "# take base case and turn it into high resolution test data - best used for contour data \n",
    "def generate_test_data(airfoil_input, le_sweep_ang, taper_ratio, semispan, alpha, mach, num_pt_side=120, num_secs=42, ):\n",
    "    if isinstance(num_pt_side, int):\n",
    "        tiling_num = 2*num_pt_side * num_secs\n",
    "    else: \n",
    "        tiling_num = (num_pt_side[0].shape[0]+num_pt_side[1].shape[0]) * num_secs\n",
    "    # Get airfoil information \n",
    "    af_coord_ = airfoil_input[0]\n",
    "    af_coord = np.tile(af_coord_.reshape((1, -1)), (tiling_num, 1)) # airfoil coordinates (56) \n",
    "    af_case_ = airfoil_input[1]\n",
    "    af_case = np.ones((tiling_num, 1)) * af_case_ # airfoil case (is a guide)\n",
    "    \n",
    "    # Get wing parametrization \n",
    "    sweep = np.ones((tiling_num, 1)) * np.deg2rad(le_sweep_ang) # leading edge sweep angle [deg as input]\n",
    "    taper = np.ones((tiling_num, 1)) * taper_ratio\n",
    "    span = np.ones((tiling_num, 1)) * semispan\n",
    "    \n",
    "    # Get operating condition \n",
    "    alph = np.ones((tiling_num, 1)) * np.deg2rad(alpha)\n",
    "    minf = np.ones((tiling_num, 1)) * mach\n",
    "    \n",
    "    # Get coordinates \n",
    "    if isinstance(num_pt_side, int):\n",
    "        xhat = np.hstack((np.flip(1.0 - 2*np.cos(np.linspace(0.0, np.pi/2, num_pt_side))), np.flip(1.0 - 2*np.cos(np.linspace(np.pi/2, 0.0 , num_pt_side))))) \n",
    "        yhat = np.hstack((np.sin(np.arccos(xhat[:num_pt_side])), -np.sin(np.arccos(xhat[num_pt_side:]))))\n",
    "        span_locs = np.flip(np.cos(np.linspace(0.0, np.pi/2, num_secs))) # cosine spacing, spanwise direction\n",
    "        span_locs_all = np.kron(span_locs, np.ones(num_pt_side*2))[:,None]\n",
    "    else: \n",
    "        xhat = np.hstack((num_pt_side[0]*2-1, num_pt_side[1]*2-1))\n",
    "        yhat = np.hstack((np.sin(np.arccos(num_pt_side[0]*2-1)), -np.sin(np.arccos(num_pt_side[1]*2-1))))\n",
    "        span_locs = np.flip(np.cos(np.linspace(0.0, np.pi/2, num_secs))) # cosine spacing, spanwise direction\n",
    "        span_locs_all = np.kron(span_locs, np.ones(num_pt_side[0].shape[0]+num_pt_side[1].shape[0]))[:,None]\n",
    "\n",
    "    xyhat = np.tile(np.hstack((xhat[:,None], yhat[:,None])), (num_secs,1)) # xhat yhat repeating per span loc \n",
    "    out_x = torch.tensor(np.hstack((af_coord, xyhat, span_locs_all, alph, minf, taper, sweep, span, af_case))) \n",
    "    return out_x\n",
    "\n",
    "# For test cases shown in paper \n",
    "# test case 1: 91\n",
    "# test case 2: 44\n",
    "# test case 3: 92\n",
    "target_case = 92 \n",
    "true_tests = select_test_case([target_case], [test_x, test_y, test_case, test_std, test_afs, test_surf])\n",
    "if output_device.type != 'cpu':\n",
    "    true_test_x, true_test_y, true_test_std, true_test_surf = true_tests[0].cuda(), true_tests[1].cuda(), true_tests[2], true_tests[3]\n",
    "else: \n",
    "    true_test_x, true_test_y, true_test_std, true_test_surf = true_tests[0], true_tests[1], true_tests[2], true_tests[3]\n",
    "\n",
    "# True Test_x\n",
    "true_test_x_u, true_test_x_l = (true_test_x[true_test_surf=='U'][:,56] + 1) / 2, (true_test_x[true_test_surf=='L'][:,56] + 1) / 2\n",
    "true_test_y_u, true_test_y_l = true_test_x[true_test_surf=='U'][:,58], true_test_x[true_test_surf=='L'][:,58]\n",
    "true_test_cp_u, true_test_cp_l = true_test_y[true_test_surf=='U'], true_test_y[true_test_surf=='L']\n",
    "true_test_ci_u, true_test_ci_l = 2*true_test_std[true_test_surf=='U'], 2*true_test_std[true_test_surf=='L']\n",
    "\n",
    "# Test-x for high resolution prediction \n",
    "# num_pts_per_side, num_span_sec = 120, 42 \n",
    "# test_AoA = 8.85\n",
    "# test_Minf = 0.13\n",
    "# test_wing_sweep  = 0.0\n",
    "# test_wing_taper  = 1.0\n",
    "# test_wing_semiAR = 2.95\n",
    "# test_wing_airfoil = [true_test_x[0, :28*2].cpu().detach().numpy(), target_case]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Airfoil Model-prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading in model...\n",
      "[DEBUG] Loading complete!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e63d623fbd946e48c2893eedd980535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run only if ... \n",
    "from large_airfoil_model_lightweight import lam_adapt # This is the updated lam_adapt with SVDKL model \n",
    "\n",
    "lam_model, lam_likelihood = lam_adapt.unpack_model(model_version='v2', output_device=output_device, verbose=True)\n",
    "\n",
    "num_per_case = 3\n",
    "num_pts_per_surface = 600\n",
    "mean_list = []\n",
    "cov_list = []\n",
    "case_iterable = tn.tqdm(all_x[:, -1].unique())\n",
    "for case in case_iterable:\n",
    "    case_train_entry_ = all_x[all_x[:, -1]==case][0]\n",
    "    # Obtain operating conditions from case \n",
    "    case_aoa_ = np.rad2deg(case_train_entry_[-6].item())\n",
    "    case_aoa_range_ = np.linspace(0.0, case_aoa_, num_per_case)\n",
    "    case_mach_ = case_train_entry_[-5].item()\n",
    "    \n",
    "    # Generate array describing airfoil geometry \n",
    "    # this entry does not scale the airfoil and goes from TE - LE - TE \n",
    "    case_xc_ = np.array([0, 0.0025, 0.0075, 0.01, 0.015, 0.02, 0.025, 0.05, 0.075, 0.1, \n",
    "                                0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
    "                                0.65, 0.70, 0.75, 0.8, 0.85, 0.90, 0.95, 1.0])\n",
    "    case_zcu_ = case_train_entry_[:28].cpu().detach().numpy()\n",
    "    case_zcl_ = case_train_entry_[28:28*2].cpu().detach().numpy()\n",
    "    case_af_ = np.hstack((np.hstack((np.flip(case_xc_), case_xc_))[:,None], \n",
    "                          np.hstack((case_zcu_, case_zcl_))[:,None]))\n",
    "    \n",
    "    pred_mu_ = []\n",
    "    pred_cov_ = []\n",
    "    # Sweep thru AoA to get the \"middle ground\" prior\n",
    "    for aoa__ in case_aoa_range_:\n",
    "        case_input__ = lam_adapt.input_data(case_af_, aoa__, case_mach_, num_auto_points=num_pts_per_surface, output_device=output_device, model_version='v2')\n",
    "        case_tensor__ = case_input__.assemble_tensor()\n",
    "        # rearrange \n",
    "        case_tensor__ = torch.vstack((torch.flip(case_tensor__[:num_pts_per_surface, :], dims=[0]), case_tensor__[num_pts_per_surface:, :]))\n",
    "        pred__ = lam_model.predict(case_tensor__,)\n",
    "        pred_mu_.append((pred__['cp_distribution'].mean.cpu().detach().numpy()-scaler_mean) * scaler_scale)\n",
    "        pred_cov_.append(pred__['cp_distribution'].covariance_matrix.cpu().detach().numpy() * scaler_scale**2)\n",
    "\n",
    "    mean_list.append(np.mean(np.stack(pred_mu_), axis=0))\n",
    "    cov_list.append(np.mean(np.stack(pred_cov_), axis=0)) \n",
    "    xhat_locs = pred__['xc'].cpu().detach().numpy()*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import positivity constraint\n",
    "from gpytorch.constraints import GreaterThan, Interval \n",
    "    \n",
    "# Make a smooth function based on a superposition of generalized logistic sigmoid functions\n",
    "def makeSmoothFx(X, vals, deltaXs, rates):\n",
    "    if len(vals) == 3:\n",
    "        smoothedFx = vals[0] + \\\n",
    "                + generalizedLogiSig(X, asym_L=0.0, asym_R=vals[1]-vals[0], rate=rates[0], shift = -deltaXs[0], nu=1) \\\n",
    "                + generalizedLogiSig(X, asym_L=0.0, asym_R=vals[2]-vals[1], rate=rates[1], shift = deltaXs[1], nu=1)  #  \n",
    "        \n",
    "    elif len(vals) == 2:\n",
    "        smoothedFx = vals[0] + \\\n",
    "                + generalizedLogiSig(X, asym_L=0.0, asym_R=vals[1]-vals[0], rate=rates[0], shift = deltaXs[0], nu=1) \n",
    "    return smoothedFx\n",
    "\n",
    "# Generalized logistic sigmoid function\n",
    "def generalizedLogiSig(X, asym_L = 0.0, asym_R = 1.0, rate = 1, shift = 0.0, nu = 1):\n",
    "    y = asym_L + (asym_R - asym_L)/(1 + torch.exp(-rate*(X-shift)))**(1/nu)\n",
    "    return y\n",
    "\n",
    "# Spacially varying Matern Kernel \n",
    "class SVH_Matern_2var_1d(gpytorch.kernels.Kernel):\n",
    "    is_stationary = True\n",
    "    def __init__(self, rates=[40], **kwargs):\n",
    "        # Initialize the base ScaleKernel\n",
    "        super().__init__(**kwargs)\n",
    "        self.rates = rates \n",
    "        # Define your custom scaling function based on x1 and x2\n",
    "        self.register_parameter(name=\"raw_ell_1\", parameter=torch.nn.Parameter(torch.tensor([1.0, 0.1]))) # two lengthscales, inboard & outboard\n",
    "        self.register_parameter(name=\"raw_var_1\", parameter=torch.nn.Parameter(torch.tensor([1.0, 1.0]))) # two vars, accordingly\n",
    "        self.register_parameter(name=\"raw_locs\", parameter=torch.nn.Parameter(torch.tensor([0.75]))) # one location of transition\n",
    "        \n",
    "        # Include Constraints\n",
    "        self.register_constraint(\"raw_ell_1\", GreaterThan(1e-2))\n",
    "        self.register_constraint(\"raw_var_1\", GreaterThan(1e-2))\n",
    "        self.register_constraint(\"raw_locs\", Interval(1e-2, 1.0))\n",
    "        # prior \n",
    "        self.register_prior(\n",
    "                \"ell_1_prior\",\n",
    "                gpytorch.priors.MultivariateNormalPrior(torch.as_tensor([1.0, 0.1]), torch.diag(torch.tensor([0.05, 0.05])**2)),\n",
    "                lambda m: m.ell_1,\n",
    "                lambda m, v : m._set_ell_1(v),)\n",
    "        self.register_prior(\n",
    "                \"var_1_prior\",\n",
    "                gpytorch.priors.MultivariateNormalPrior(torch.as_tensor([1.0, 1.0]), torch.eye(2)*1**2),  # torch.eye(2)*1\n",
    "                lambda m: m.var_1,\n",
    "                lambda m, v : m._set_var_1(v),)\n",
    "        self.register_prior( # This is not used as of now \n",
    "                \"locs_prior\",\n",
    "                gpytorch.priors.NormalPrior(0.75, 0.1), \n",
    "                lambda m: m.locs,\n",
    "                lambda m, v : m._set_locs(v),)\n",
    "\n",
    "    @property\n",
    "    def ell_1(self):\n",
    "        return self.raw_ell_1_constraint.transform(self.raw_ell_1)\n",
    "    @ell_1.setter\n",
    "    def ell_1(self, values):\n",
    "        return self._set_ell_1(values)\n",
    "    def _set_ell_1(self, values):\n",
    "        if not torch.is_tensor(values):\n",
    "            values = torch.as_tensor(values).to(self.ell_1)\n",
    "        self.initialize(raw_ell_1=self.raw_ell_1_constraint.inverse_transform(values))\n",
    "        \n",
    "    @property\n",
    "    def var_1(self):\n",
    "        return self.raw_var_1_constraint.transform(self.raw_var_1)\n",
    "    @var_1.setter\n",
    "    def var_1(self, value):\n",
    "        return self._set_var_1(value)\n",
    "    def _set_var_1(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.var_1)\n",
    "        self.initialize(raw_var_1=self.raw_var_1_constraint.inverse_transform(value))    \n",
    "    \n",
    "    @property\n",
    "    def locs(self):\n",
    "        return self.raw_locs_constraint.transform(self.raw_locs)\n",
    "    @locs.setter\n",
    "    def locs(self, values):\n",
    "        return self._set_locs(values)\n",
    "    def _set_locs(self, values):\n",
    "        if not torch.is_tensor(values):\n",
    "            values = torch.as_tensor(values).to(self.locs)\n",
    "        self.initialize(raw_locs=self.raw_locs_constraint.inverse_transform(values))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        spatially_varying_length_sq = self.spatially_varying_function(x1, x2, self.ell_1)\n",
    "        spatially_varying_scale = self.spatially_varying_function(x1, x2, self.var_1)\n",
    "        xhat_dist = self.covar_dist(x1, x2)\n",
    "        \n",
    "        # Compute the Matern 5/2 kernel function\n",
    "        # k(x, x') = (1 + sqrt(5) * d / l + 5 / 3 * (d / l)^2) * exp(-sqrt(5) * d / l)\n",
    "        xdist_l = xhat_dist/spatially_varying_length_sq.pow(0.5)\n",
    "        term1_1 = 1 + torch.sqrt(torch.tensor(3.0))*xdist_l  \n",
    "        term1_2 = torch.exp(-torch.sqrt(torch.tensor(3.0))*xdist_l)\n",
    "        out = term1_1*term1_2* spatially_varying_scale \n",
    "        return out \n",
    "\n",
    "    def spatially_varying_function(self, x1, x2, hyperparams):\n",
    "        x1 = x1.flatten()\n",
    "        x2 = x2.flatten()\n",
    "        # Define a custom scale function that depends on x1 and x2\n",
    "        val_x1_1d = torch.zeros((x1.shape[0],))\n",
    "        val_x2_1d = torch.zeros((x2.shape[0],))\n",
    "        \n",
    "        # Apply the piecewise conditions for the scale function\n",
    "        val_x1_1d = makeSmoothFx(x1, hyperparams, [0.7], self.rates) \n",
    "        val_x2_1d = makeSmoothFx(x2, hyperparams, [0.7], self.rates)  \n",
    "        val_ex1 = val_x1_1d.reshape((-1,1)).tile((1, x2.shape[0]))\n",
    "        val_ex2 = val_x2_1d.reshape((1,-1)).tile((x1.shape[0], 1))\n",
    "        \n",
    "        spatially_varying = val_ex1 * val_ex2\n",
    "        return spatially_varying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import tqdm.notebook as tn  \n",
    "import pickle\n",
    "from scipy.interpolate import RegularGridInterpolator, CubicSpline\n",
    "\"\"\" \n",
    "Covariance class \n",
    "\"\"\"\n",
    "class lam_prior_cov(gpytorch.kernels.Kernel):\n",
    "    is_stationary = True\n",
    "    def __init__(self, covar_interpolator, train_x, guide, output_device='cpu', \n",
    "                 verbose=False, active_dims=torch.tensor([56, 57, 58, 64], dtype=int), **kwargs):\n",
    "        # Initialize the base ScaleKernel\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_device = output_device # output device, cpu or cuda\n",
    "        self.interpolator = covar_interpolator # covariance interpolator \n",
    "        self.active_dims = active_dims.to(self.output_device) \n",
    "        self.register_buffer(\"active_dims\", self.active_dims)\n",
    "        self.train_x = train_x # training data to use as reference \n",
    "        self.case_guide = torch.from_numpy(guide).cpu() # case guide\n",
    "        self.verbose = verbose # show logs or nah\n",
    "        self.train_cache = None  # cache for training data covariance \n",
    "        # Cache the covariance matrix for training data  \n",
    "        print_with_tag('Computing cache for training data covariance matrix.', 'LOG') if self.verbose else None \n",
    "        self.train_cache = self(self.train_x).evaluate()\n",
    "    \n",
    "    # __call__ calls this with lazy evaluation\n",
    "    \"\"\" Evaluate the LAM-prior covariance matrix to obtain K_{x1. x2}\n",
    "    x1 and x2 should follow:\n",
    "    [xhat, yhat, spanwise location, case_number] \n",
    "    \"\"\"\n",
    "    # this runs at least x2 faster on CPU \n",
    "    def forward_(self, x1, x2, diag=False, **params):   \n",
    "        # If cache exists and we are computing training data covar, Kxx\n",
    "        if self.train_cache is not None and x1.shape[0] == self.train_x.shape[0] and x1.shape[0] == x2.shape[0]:\n",
    "            # If the covariance is already cached, use it\n",
    "            # print_with_tag('Reading in cache for training data covariance matrix.', 'LOG') if self.verbose else None \n",
    "            K = self.train_cache\n",
    "        else: \n",
    "            # K = [[A  , ..., B  ]\n",
    "            #      [..., ..., ...]\n",
    "            #      [C  , ..., D  ]]\n",
    "            # obtain unique spanwise locations \n",
    "            x1_ = x1.cpu()\n",
    "            x2_ = x2.cpu()\n",
    "            # Compute the covariance matrix using the interpolator \n",
    "            x1_unique_case = torch.unique(x1_[:,3])\n",
    "            x2_unique_case = torch.unique(x2_[:,3]) \n",
    "            k_row = [] # covariance matrix row-wise piece (i.e. [A, ..., B] or [C, ..., D]) \n",
    "            with torch.no_grad():\n",
    "                # Iterate through case by case \n",
    "                \n",
    "                for i in x1_unique_case:   \n",
    "                    \n",
    "                    k_block = [] # represents individual blocks made from the \"case\" (i.e. A, B, C, D)\n",
    "                    for j in x2_unique_case: \n",
    "                        # Obtain relevant location within the row \n",
    "                        target_span_x1_idx = torch.argmin(torch.abs(torch.as_tensor(self.case_guide) - i))\n",
    "                        target_span_x2_idx = torch.argmin(torch.abs(torch.as_tensor(self.case_guide) - j))\n",
    "                        \n",
    "                        # subsample from x1 and x2 the relevant cases \n",
    "                        x1_subsample = x1_[x1_[:,3]==i].clone()\n",
    "                        x2_subsample = x2_[x2_[:,3]==j].clone()\n",
    "                        \n",
    "                        # Get the spanwise locations from each case\n",
    "                        x1_unique_span = torch.unique(x1_subsample[:,2])\n",
    "                        x2_unique_span = torch.unique(x2_subsample[:,2]) \n",
    "                        k_segment = [] # represents individual spanwise locations within each case \n",
    "                        for u in x1_unique_span: \n",
    "                            k_subsegment = []\n",
    "                            for v in x2_unique_span:\n",
    "                                x1_subsubsample = x1_subsample[x1_subsample[:,2]==u].clone()\n",
    "                                x2_subsubsample = x2_subsample[x2_subsample[:,2]==v].clone()\n",
    "                                \n",
    "                                # Take spanwise locations, convert to non-dimensional chordwise locations (x/c) from xhat yhat \n",
    "                                x1_subsubsample[x1_subsubsample[:,1] >= 0.0, 0] = -(x1_subsubsample[x1_subsubsample[:,1] >= 0.0, 0] + 1)/2.0\n",
    "                                x1_subsubsample[x1_subsubsample[:,1]  < 0.0, 0] =  (x1_subsubsample[x1_subsubsample[:,1]  < 0.0, 0] + 1)/2.0\n",
    "                                \n",
    "                                x2_subsubsample[x2_subsubsample[:,1] >= 0.0, 0] = -(x2_subsubsample[x2_subsubsample[:,1] >= 0.0, 0] + 1)/2.0\n",
    "                                x2_subsubsample[x2_subsubsample[:,1]  < 0.0, 0] =  (x2_subsubsample[x2_subsubsample[:,1]  < 0.0, 0] + 1)/2.0\n",
    "                                    \n",
    "                                x1x1, x2x2 = np.meshgrid(x1_subsubsample[:,0].cpu(), x2_subsubsample[:,0].cpu())  \n",
    "\n",
    "                                # Evaluate the covariance matrix using the interpolator \n",
    "                                if u == v: # Diagonal elements \n",
    "                                    k_subsegment.append(torch.tensor(self.interpolator[target_span_x2_idx]((x2x2, x1x1))).T)\n",
    "                                else:  # assume independence b/n cases \n",
    "                                    k_subsegment.append(torch.zeros((x1_subsubsample.shape[0], x2_subsubsample.shape[0])))\n",
    "                            # add 'em to be concated a single block\n",
    "                            k_segment.append(torch.hstack(k_subsegment)) \n",
    "                        # concatenate the each spanwise segment into a single block\n",
    "                        if i == j: \n",
    "                            k_block.append(torch.cat(k_segment))\n",
    "                        else:  \n",
    "                            k_block.append(torch.zeros((x1_subsample.shape[0], x2_subsample.shape[0])))\n",
    "                    # append each element of the row into a single row \n",
    "                    k_row.append(torch.hstack(k_block)) \n",
    "                # concatenate the rows into a single covariance matrix \n",
    "                K = torch.cat(k_row)\n",
    "                K = K.to(self.output_device)\n",
    "        return K \n",
    "\n",
    "    # ...existing code...\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Use cache if possible\n",
    "        if (\n",
    "            self.train_cache is not None\n",
    "            and x1.shape[0] == self.train_x.shape[0]\n",
    "            and x1.shape[0] == x2.shape[0]\n",
    "        ):\n",
    "            return self.train_cache\n",
    "\n",
    "        x1_ = x1.cpu()\n",
    "        x2_ = x2.cpu()\n",
    "        case_guide = self.case_guide\n",
    "\n",
    "        # Get unique cases and spanwise locations, and their indices\n",
    "        x1_cases, x1_case_idx = torch.unique(x1_[:, 3], return_inverse=True)\n",
    "        x2_cases, x2_case_idx = torch.unique(x2_[:, 3], return_inverse=True)\n",
    "        x1_spans, x1_span_idx = torch.unique(x1_[:, 2], return_inverse=True)\n",
    "        x2_spans, x2_span_idx = torch.unique(x2_[:, 2], return_inverse=True)\n",
    "\n",
    "        # Precompute the mapping from (case, span) to row indices\n",
    "        x1_case_span = torch.stack([x1_case_idx, x1_span_idx], dim=1)\n",
    "        x2_case_span = torch.stack([x2_case_idx, x2_span_idx], dim=1)\n",
    "\n",
    "        # Build a lookup for each (case, span) group\n",
    "        def build_group_lookup(unique_cases, unique_spans, case_idx, span_idx):\n",
    "            group_lookup = {}\n",
    "            for i, case in enumerate(unique_cases):\n",
    "                for j, span in enumerate(unique_spans):\n",
    "                    idx = (case_idx == i) & (span_idx == j)\n",
    "                    if idx.any():\n",
    "                        group_lookup[(case.item(), span.item())] = idx.nonzero(as_tuple=True)[0]\n",
    "            return group_lookup\n",
    "\n",
    "        x1_lookup = build_group_lookup(x1_cases, x1_spans, x1_case_idx, x1_span_idx)\n",
    "        x2_lookup = build_group_lookup(x2_cases, x2_spans, x2_case_idx, x2_span_idx)\n",
    "\n",
    "        # Preallocate output\n",
    "        K = torch.zeros(x1_.shape[0], x2_.shape[0], device='cpu')\n",
    "\n",
    "        # For each block (case, span) pair, fill in the covariance\n",
    "        with torch.no_grad():\n",
    "            for (case1, span1), idx1 in x1_lookup.items():\n",
    "                for (case2, span2), idx2 in x2_lookup.items():\n",
    "                    # Only fill diagonal blocks (same case and span)\n",
    "                    if (case1 == case2) and (span1 == span2):\n",
    "                        # Get indices for the case in the guide\n",
    "                        target_span_idx = torch.argmin(torch.abs(case_guide - case1)).item()\n",
    "                        # Extract sub-blocks\n",
    "                        x1_block = x1_[idx1]\n",
    "                        x2_block = x2_[idx2]\n",
    "                        # Convert xhat as in original code\n",
    "                        x1_xhat = x1_block[:, 0].clone()\n",
    "                        x2_xhat = x2_block[:, 0].clone()\n",
    "                        x1_yhat = x1_block[:, 1]\n",
    "                        x2_yhat = x2_block[:, 1]\n",
    "                        # Vectorized transformation\n",
    "                        x1_xhat[x1_yhat >= 0.0] = -(x1_xhat[x1_yhat >= 0.0] + 1) / 2.0\n",
    "                        x1_xhat[x1_yhat < 0.0] = (x1_xhat[x1_yhat < 0.0] + 1) / 2.0\n",
    "                        x2_xhat[x2_yhat >= 0.0] = -(x2_xhat[x2_yhat >= 0.0] + 1) / 2.0\n",
    "                        x2_xhat[x2_yhat < 0.0] = (x2_xhat[x2_yhat < 0.0] + 1) / 2.0\n",
    "                        # Meshgrid for all pairs\n",
    "                        X1, X2 = torch.meshgrid(x1_xhat, x2_xhat, indexing='ij')\n",
    "                        # Evaluate covariance\n",
    "                        K_block = torch.tensor(\n",
    "                            self.interpolator[target_span_idx]((X2.cpu().numpy(), X1.cpu().numpy()))\n",
    "                        ).T\n",
    "                        # Assign to output\n",
    "                        K[idx1[:, None], idx2] = K_block\n",
    "                    else:\n",
    "                        # Off-diagonal blocks remain zero (already initialized)\n",
    "                        continue\n",
    "\n",
    "        return K.to(self.output_device)\n",
    "# ...existing code...\n",
    "from copy import deepcopy\n",
    "\n",
    "\"\"\"\n",
    "mean class \n",
    "\"\"\"\n",
    "class lam_prior_mean(gpytorch.means.Mean):\n",
    "    def __init__(self, mean_interpolator, train_x, guide, output_device='cpu', \n",
    "                 verbose=False, **kwargs):\n",
    "        # Initialize the base ScaleKernel\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_device = output_device # output device, cpu or cuda\n",
    "        self.interpolator = mean_interpolator # mean interpolator \n",
    "        self.train_x = train_x # training data to use as reference \n",
    "        self.case_guide = torch.from_numpy(guide).cpu() # case guide\n",
    "        self.verbose = verbose # show logs or nah\n",
    "        self.train_cache = None  # cache for training data covariance \n",
    "        # Cache the covariance matrix for training data  \n",
    "        print_with_tag('Computing cache for training data mean.', 'LOG') if self.verbose else None \n",
    "        self.train_cache = self(self.train_x)\n",
    "        \n",
    "    def forward(self, x):  \n",
    "        x_ = x.cpu()\n",
    "        with torch.no_grad():\n",
    "            # Operating on training data \n",
    "            if x_.shape[0] == self.train_x.shape[0]:\n",
    "                if self.train_cache is None:\n",
    "                    # calculate for training data \n",
    "                    x_unique_case = torch.unique(x_[:,-1])  \n",
    "                    mu_ = []\n",
    "                    for i in x_unique_case:\n",
    "                        target_span_idx = torch.argmin(torch.abs(torch.as_tensor(self.case_guide) - i))\n",
    "                        x_subsample = x_[x_[:,-1]==i][:, 56:56+2+1].clone()\n",
    "                        # print(x_subsample)\n",
    "                        x_subsample[x_subsample[:,1]>=0.0, 0] = -(x_subsample[x_subsample[:,1]>=0.0, 0]+1)/2\n",
    "                        x_subsample[x_subsample[:,1] <0.0, 0] =  (x_subsample[x_subsample[:,1] <0.0, 0]+1)/2\n",
    "                        mu_.append(torch.tensor(self.interpolator[target_span_idx](x_subsample[:,0])))\n",
    "                    mu = torch.cat(mu_)\n",
    "                else: \n",
    "                    # retrieve cached value \n",
    "                    mu = self.train_cache\n",
    "            else: \n",
    "                # calcualte to get mu\n",
    "                # xx = x_[:self.train_x.shape[0]]\n",
    "                # ss = x_[self.train_x.shape[0]:]\n",
    "                \n",
    "                # xx_unique_case = torch.unique(xx[:,-1]) # unique case \n",
    "                # mu_x = []\n",
    "                # for i in xx_unique_case:\n",
    "                #     target_span_idx = torch.argmin(torch.abs(torch.as_tensor(self.case_guide) - i))\n",
    "                #     xx_subsample = x_[x_[:,-1]==i][:, 56:56+2+1].clone() #deepcopy(xx[xx[:,-1]==i])\n",
    "                    \n",
    "                #     xx_subsample[xx_subsample[:,1]>=0.0, 0] = -(xx_subsample[xx_subsample[:,1]>=0.0, 0]+1)/2\n",
    "                #     xx_subsample[xx_subsample[:,1]< 0.0, 0] =  (xx_subsample[xx_subsample[:,1]< 0.0, 0]+1)/2 \n",
    "                #     mu_x.append(torch.tensor(self.interpolator[target_span_idx](xx_subsample[:,0]))) \n",
    "                    \n",
    "                ss_unique_case = torch.unique(x_[:,-1]) # unique case \n",
    "                mu_s = []\n",
    "                for j in ss_unique_case:\n",
    "                    target_span_idx = torch.argmin(torch.abs(torch.as_tensor(self.case_guide) - j))\n",
    "                    ss_subsample = x_[x_[:,-1]==j][:, 56:56+2+1].clone()\n",
    "                    \n",
    "                    ss_subsample[ss_subsample[:,1]>=0.0, 0] = -(ss_subsample[ss_subsample[:,1]>=0.0, 0]+1)/2\n",
    "                    ss_subsample[ss_subsample[:,1]< 0.0, 0] =  (ss_subsample[ss_subsample[:,1]< 0.0, 0]+1)/2\n",
    "                    mu_s.append(torch.tensor(self.interpolator[target_span_idx](ss_subsample[:,0])))\n",
    "                mu = torch.cat(mu_s).flatten()\n",
    "                #torch.cat((torch.cat(mu_x).reshape((-1,1)), torch.cat(mu_s).reshape((-1,1)))).flatten()\n",
    "        \n",
    "            mu = mu.flatten().to(self.output_device)\n",
    "        return mu    \n",
    "    \n",
    "\"\"\"\n",
    "Class to manage both of them at once \n",
    "\"\"\"\n",
    "class lam_prior():\n",
    "    def __init__(self, priors, case_guides, output_device='cpu', verbose=False):\n",
    "        self.verbose = verbose\n",
    "        if isinstance(priors, str): # if providing directory\n",
    "            with open(priors, 'rb') as file:\n",
    "                pickle_loaded = pickle.load(file)\n",
    "                prior_mean_list = pickle_loaded['prior_mean']\n",
    "                prior_cov_list = pickle_loaded['prior_cov']\n",
    "                updated_inputs = pickle_loaded['ex_input'][0][:, -2]\n",
    "        elif isinstance(priors, list): # if providing list \n",
    "            prior_mean_list = priors[0]\n",
    "            prior_cov_list = priors[1]\n",
    "            updated_inputs = priors[2]\n",
    "        else: \n",
    "            raise ValueError('invalid input')\n",
    "        # This needs to be streamlined later \n",
    "        mu_interpolator, covar_interpolator, cases = self.create_interpolators(prior_mean_list, prior_cov_list, [updated_inputs[:num_pts_per_surface], updated_inputs[num_pts_per_surface:]], case_guides)\n",
    "\n",
    "        self.mean_module = lam_prior_mean(mu_interpolator, train_x, cases, output_device=output_device, verbose=self.verbose)\n",
    "        self.covar_module = lam_prior_cov(covar_interpolator, train_x, cases, output_device=output_device, verbose=self.verbose)\n",
    "        \n",
    "    \"\"\"\n",
    "    Creates interpolator for the mean and covariance for LAM-prior\n",
    "    Doing this rigourously would require calling int the LAM (will be implemented later)\n",
    "    For simplicity, the priors for each case is precomputed using the LAM and and interpolating appropriately on the test data\n",
    "    mean_list: list of mean values for LAM predictions\n",
    "    covar_list: list of covariance matrices for LAM predictions\n",
    "    xhat_locs: xhat locations for the upper and lower surfaces \n",
    "    cases: case number to use as guides \n",
    "    \"\"\"\n",
    "    def create_interpolators(self, mean_list, covar_list, xhat_locs, cases):\n",
    "        # Convert xhat locations to non-dimensional chordwise directions\n",
    "        xc_u = (xhat_locs[0]+1)/2 # Upper surface\n",
    "        xc_l = (xhat_locs[1]+1)/2 # Lower surface\n",
    "        mu_interp, cov_interp = [], [] \n",
    "        for i in range(0, len(cases)):\n",
    "            # unravel such that it goes from \n",
    "            xc_unravel = np.hstack((-xc_u[:-1], xc_l))\n",
    "            \n",
    "            # get rid of the repeating zero within this data \n",
    "            popped_mean = np.delete(mean_list[i], xc_u.shape[0], 0)\n",
    "            popped_covar = np.delete(covar_list[i], xc_u.shape[0], 0)\n",
    "            popped_covar = np.delete(popped_covar, xc_u.shape[0], 1)\n",
    "\n",
    "            # Create interpolator\n",
    "            mu_interp.append(CubicSpline(xc_unravel, popped_mean))\n",
    "            cov_interp.append(RegularGridInterpolator((xc_unravel, xc_unravel), popped_covar))\n",
    "            case_guide = cases\n",
    "        return mu_interp, cov_interp, case_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[56, xh, yh, yb, a, m, l, Lambda, ar, case]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kernel_functions import SVH_Matern_2var_1d\n",
    "# Define model \n",
    "\n",
    "# Wrapper for convenience \n",
    "class large_wing_model():\n",
    "    def __init__(self, model, likelihood, optimizer, scheduler, verbose=False, output_device='cpu'):\n",
    "        # I want to have DKL_GP in here with optimizer and stuff\n",
    "        self.output_device = output_device\n",
    "        self.model = model.to(output_device)\n",
    "        self.likelihood = likelihood.to(output_device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.history = {'loss': [],\n",
    "                        'validation_err': [],\n",
    "                        'checkpoints': []}\n",
    "        self.save_directory = './'\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def train_model(self, training_iterations, save_name, save_interval=200):\n",
    "        # Set up loss function - Marginal Log Likelihood \n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)  \n",
    "        # from soft_dtw_cuda import SoftDTW\n",
    "        # sdtw = SoftDTW(use_cuda=True, gamma=0.1)\n",
    "        checkpt_ct = 1 \n",
    "        sub_iter = 0 \n",
    "        main_iter = 0 \n",
    "        iterator = tn.tqdm(range(training_iterations))\n",
    "        for i in iterator: \n",
    "            self.model.train()\n",
    "            self.likelihood.train()\n",
    "            \n",
    "            # Zero backprop gradients\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            # Get output from model\n",
    "            output = self.model(train_x)\n",
    "            \n",
    "            # Calculate loss \n",
    "            loss = -mll(output, train_y)  \n",
    "            loss.backward()\n",
    "            iterator.set_postfix(loss=loss.item())\n",
    "            self.history['loss'].append(loss.item())\n",
    "            \n",
    "            if sub_iter >= save_interval-1:\n",
    "                with torch.no_grad():\n",
    "                    val_pred = self.predict(test_wing=true_test_x.to(self.output_device), save_to=None) \n",
    "                    # Use the sdtw from https://github.com/Maghoumi/pytorch-softdtw-cuda if ya wants\n",
    "                    val_loss = 0.0 #sdtw(true_test_y.reshape((1, -1, 1)).cuda(), val_pred[0].mean.reshape((1, -1, 1)).cuda())\n",
    "                    self.history['validation_err'].append(val_loss)  # .item()\n",
    "                    val_loss_2  = ((true_test_y.cuda() - val_pred[0].mean.cuda())**2).sum()\n",
    "                    a = np.trapezoid(np.abs(true_test_y.cpu().detach().numpy()))\n",
    "                    b = np.trapezoid(np.abs(val_pred[0].mean.cpu().detach().numpy()))\n",
    "                    print(f'Checkpoint {checkpt_ct} validation loss: {np.round(val_loss, 2)}, {np.round(np.abs(a-b), 2)}, {np.round(val_loss_2.item(), 2)}') # .item()\n",
    "                self.history['checkpoints'].append(save_name + str(checkpt_ct)) \n",
    "                self.save_checkpoint(self.save_directory + self.history['checkpoints'][-1])\n",
    "                checkpt_ct += 1\n",
    "                sub_iter = 0  \n",
    "            \n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            sub_iter += 1 \n",
    "         \n",
    "    \"\"\" Save checkpoint to user-provided file_path \"\"\"\n",
    "    def save_checkpoint(self, file_path): \n",
    "        torch.save({\n",
    "        'model_state_dict': self.model.state_dict(),\n",
    "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        'likelihood': self.likelihood.state_dict()\n",
    "        }, file_path)\n",
    "         \n",
    "    \n",
    "    \"\"\" Load saved checkpoint from user-provided file_path\"\"\"\n",
    "    def load_checkpoint(self, file_path):\n",
    "        loaded_ = torch.load(file_path, weights_only=False, map_location='cpu')\n",
    "        self.model.load_state_dict(loaded_['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(loaded_['optimizer_state_dict'])\n",
    "        self.likelihood.load_state_dict(loaded_['likelihood']) \n",
    "        print_with_tag(f'Loading weight: {file_path}', 'LWM') if self.verbose else None\n",
    "\n",
    "    def _gen_save_string(prefix, lr, decay, nn_dims, losses, iters, override_date=None):\n",
    "        from datetime import date\n",
    "        if override_date is not None:\n",
    "            save_str_date = override_date\n",
    "        else:\n",
    "            today = date.today()\n",
    "            save_str_date = today.strftime(\"%Y%m%d\") + '_'\n",
    "        save_str_prefix = prefix + '_' \n",
    "        save_str_optimizer = 'lr' + str(lr) + '_' + 'decay' + str(decay) + '_'\n",
    "        save_str_dims = 'dims' + str(nn_dims[0]) + '-' + str(nn_dims[1]) + '-' + str(nn_dims[2]) + '-' + str(nn_dims[3]) + '_'\n",
    "        save_str_checkpt = 'checkpt' + str(iters) + '_'\n",
    "        save_str_loss = 'loss' + str(losses[0]) + '_'\n",
    "        save_str_trainmetric = 'trainMetric' + str(losses[1]) + '_'\n",
    "        save_str_testmetric = 'testLoss' + str(losses[2])\n",
    "        return save_str_date + save_str_checkpt + save_str_prefix + save_str_optimizer + save_str_dims + save_str_loss + save_str_trainmetric + save_str_testmetric \n",
    "    \n",
    "    \"\"\"\n",
    "    Generate predictions from the model;\n",
    "    Manual implementation with LAM priors\n",
    "    \"\"\"\n",
    "    def predict(self, test_wing, target_weights:list=[None], condition={'cl': None}, save_to:str=None, get_runtime=False):\n",
    "        \n",
    "        \n",
    "        # Assemble input tensor from wing      \n",
    "        if isinstance(test_wing, input_wing):\n",
    "            test_data_list = test_wing.assemble_tensor()\n",
    "        elif isinstance(test_wing, torch.Tensor): \n",
    "            test_data_list = [test_wing]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input wing\")\n",
    "        \n",
    "        output_distr = [] \n",
    "        \n",
    "        \n",
    "        self.model.eval() \n",
    "        self.likelihood.eval()\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for test_idx, test_data in enumerate(test_data_list):\n",
    "            print_with_tag('Generating prediction...', 'LWM') if self.verbose else None \n",
    "            # Get relevant prior \n",
    "            if self.model.lam_priors is not None: # if using LAM prior \n",
    "                Kxx_prior = self.model.prior_cov.train_cache \n",
    "                Kxs_prior = self.model.prior_cov(train_x, test_data).to(self.output_device).evaluate() #.evaluate()#.to(self.output_device)\n",
    "                Kss_prior = self.model.prior_cov(test_data).to(self.output_device).evaluate() #.evaluate()#.to(self.output_device) \n",
    "                muxx_prior = self.model.mean_module.train_cache #.to(self.output_device)#.cpu().detach().numpy() \n",
    "                muss_prior = self.model.mean_module(test_data) #.to(self.output_device)#.cpu().detach().numpy() \n",
    "            else:  \n",
    "                print(\"Error.... no prior\")\n",
    "                \n",
    "            with torch.no_grad():   \n",
    "                torch.cuda.empty_cache()\n",
    "                # Get average from weights\n",
    "                pred_mu = torch.zeros(test_data.shape[0]).to(self.output_device)\n",
    "                pred_cov = torch.zeros(test_data.shape[0], test_data.shape[0]).to(self.output_device)\n",
    "                for weight in target_weights:\n",
    "                    if weight is not None: \n",
    "                        self.load_checkpoint(weight)\n",
    "                        self.model.eval()\n",
    "                        self.likelihood.eval()\n",
    "                        \n",
    "                    # Need to optimize?\n",
    "                    def _project_x(x):\n",
    "                        with torch.no_grad():   \n",
    "                            proj_x = self.model.feature_extractor(torch.hstack((x[:,:58], x[:,59:-1]))) # do without eta \n",
    "                            proj_x = self.model.scale_to_bounds(proj_x)  # Make the NN values \"nice\" \n",
    "                            proj_x = torch.hstack((proj_x, x[:, 58:59])) # re-attach eta\n",
    "                            return proj_x \n",
    "                    \n",
    "                    # Feed train and test data thru m\n",
    "                    proj_xx = _project_x(train_x)\n",
    "                    proj_ss = _project_x(test_data)\n",
    "                        \n",
    "                    Kxx_base = self.model.covar_module(proj_xx).to(self.output_device).evaluate()\n",
    "                    Kxs_base = self.model.covar_module(proj_xx, proj_ss).to(self.output_device).evaluate()\n",
    "                    Kss_base = self.model.covar_module(proj_ss).to(self.output_device).evaluate() \n",
    "\n",
    "                    Kxx = Kxx_base + Kxx_prior  #.cpu().detach().numpy()\n",
    "                    Kxs = Kxs_base + Kxs_prior #.cpu().detach().numpy()\n",
    "                    Kss = Kss_base + Kss_prior #.cpu().detach().numpy()\n",
    "                    jitter = torch.eye(Kxs.shape[0]).to(self.output_device)*1e-4\n",
    "                    \n",
    "                    if len(self.likelihood.noise) == 1:\n",
    "                        sigma = torch.eye(Kxs.shape[0]).to(self.output_device)*self.likelihood.noise.item()\n",
    "                    else: \n",
    "                        sigma = torch.diag(self.likelihood.noise).to(self.output_device)\n",
    "                    \n",
    "                    Kxx = (Kxx + Kxx.T)/2# symmetrize the matrix\n",
    "                    L = torch.linalg.cholesky(Kxx + sigma + jitter, upper=False)\n",
    "                    s1 = torch.linalg.solve_triangular(L, (train_y - muxx_prior).reshape((-1,1)), upper=False).T\n",
    "  \n",
    "                    s2 = torch.linalg.solve_triangular(L, Kxs, upper=False) \n",
    "                    \n",
    "                    pred_mu += (s1 @ s2).flatten() + muss_prior.flatten()\n",
    "                    pred_cov += Kss - s2.T @ s2\n",
    "                    # Stabilize pred_cov\n",
    "                    # pred_cov = (pred_cov + pred_cov.T)/2 \n",
    "                pred_mu /= len(target_weights)\n",
    "                pred_cov /= len(target_weights)\n",
    "                output_distr.append(gpytorch.distributions.MultivariateNormal(pred_mu.cpu()/scaler_scale + scaler_mean, pred_cov.cpu()/scaler_scale**2 + torch.eye(pred_cov.shape[0]).cpu()*1e-4))\n",
    "                end_time = time.time()\n",
    "                print('runtime:', end_time - start_time)\n",
    "                if isinstance(test_wing, input_wing):\n",
    "                    if test_wing.input_type == 'mesh':\n",
    "                        test_wing.mesh['zones_data'][test_idx][:, -2] = pred_mu.flatten().cpu().detach().numpy()/scaler_scale + scaler_mean # Assign mean value \n",
    "                        test_wing.mesh['zones_data'][test_idx][:, -1] = 2*np.sqrt(np.diag(pred_cov.cpu().detach().numpy())).flatten()/scaler_scale # Assign confidence interval \n",
    "                \n",
    "                # Make sure that this is compatible with mesh style later \n",
    "                # right now this is only compatible with grid style\n",
    "                if condition['cl'] is not None: # Expand this later   \n",
    "                    # any condition should be defined as a [mean, st_dev]\n",
    "                    condition_mean = condition['cl'][0] # target mean\n",
    "                    condition_stdev = condition['cl'][1] # target std\n",
    "                    print_with_tag(f'Constraining posterior to C_L = N({condition_mean}, {condition_stdev}^2)', 'LWM') if self.verbose else None \n",
    "                    \n",
    "                    # Establish distributions\n",
    "                    # Target distribution - constraint \n",
    "                    target_dist = torch.distributions.normal.Normal(loc=condition_mean, scale=condition_stdev)   \n",
    "                    # generate samples to generate a single proposal distribution \n",
    "                    xhat_arr = test_wing.xhat.reshape((-1, test_wing.chord_resolution*2)) # xhat location, reshaped into grid \n",
    "                    eta_arr = test_wing.eta.reshape((-1, test_wing.chord_resolution*2)) # eta location, reshaped into grid \n",
    "                    accumulated_CL_samples = []\n",
    "                    accumulated_cp_samples = []\n",
    "                    accumulated_log_weights = []\n",
    "                    cp_samples = output_distr[-1].rsample(torch.Size([10000]))\n",
    "                    cp_samples_ = cp_samples.reshape((10000, -1, test_wing.chord_resolution*2)).detach().numpy()\n",
    "                    cl_samples = -np.trapezoid(cp_samples_[:, :, :test_wing.chord_resolution], x=(xhat_arr[0][:test_wing.chord_resolution]+1)/2, axis=2) + \\\n",
    "                                    np.trapezoid(cp_samples_[:, :, test_wing.chord_resolution:], x=(xhat_arr[0][test_wing.chord_resolution:]+1)/2, axis=2)\n",
    "                    CL_samples = np.trapezoid(cl_samples, x=eta_arr[:, 0], axis=1)\n",
    "                    # Proposal distribution - original posterior \n",
    "                    proposal_dist = torch.distributions.Normal(loc=np.mean(CL_samples), scale=np.std(CL_samples))\n",
    "                    print(np.mean(CL_samples), np.std(CL_samples))\n",
    "                    # Set up sampling iterator \n",
    "                    # --- REJECTION SAMPLING INSTEAD OF IMPORTANCE SAMPLING ---\n",
    "                    filtered_cp_samples = []\n",
    "                    filtered_cl_samples = []\n",
    "                    filtered_CL_samples = []\n",
    "\n",
    "                    desired_samples = condition['num_samples']\n",
    "                    max_attempts = 1000  # To avoid infinite loops\n",
    "                    attempts = 0\n",
    "                    tqdm_iterable = tn.tqdm(range(max_attempts))\n",
    "                    for i in tqdm_iterable:  \n",
    "                        batch_size = 2000\n",
    "                        cp_samples = output_distr[-1].rsample(torch.Size([batch_size]))\n",
    "                        cp_samples_ = cp_samples.reshape((batch_size, -1, test_wing.chord_resolution*2)).detach().numpy()\n",
    "                        cl_samples = -np.trapezoid(cp_samples_[:, :, :test_wing.chord_resolution], x=(xhat_arr[0][:test_wing.chord_resolution]+1)/2, axis=2) + \\\n",
    "                                        np.trapezoid(cp_samples_[:, :, test_wing.chord_resolution:], x=(xhat_arr[0][test_wing.chord_resolution:]+1)/2, axis=2)\n",
    "                        CL_samples = np.trapezoid(cl_samples, x=eta_arr[:, 0], axis=1) \n",
    "                        # Compute acceptance probabilities (normalized to max=1)\n",
    "    \n",
    "                        target_probs = torch.exp(target_dist.log_prob(torch.from_numpy(CL_samples))).detach().numpy()\n",
    "                        proposal_prob = torch.exp(proposal_dist.log_prob(torch.from_numpy(CL_samples))).detach().numpy()\n",
    "                        # max_prob = np.max(target_probs)\n",
    "                        # acceptance_probs = target_probs / proposal_prob #max_prob\n",
    "                        ratios = target_probs / proposal_prob\n",
    "                        M = np.max(ratios)\n",
    "                        acceptance_probs = ratios / M \n",
    "\n",
    "                        # Draw uniform random numbers for acceptance\n",
    "                        random_vals = np.random.uniform(0, 1, size=batch_size)\n",
    "                        accept_mask = random_vals < acceptance_probs\n",
    "\n",
    "                        # Store accepted samples\n",
    "                        filtered_cp_samples.extend(cp_samples[accept_mask])\n",
    "                        filtered_cl_samples.extend(cl_samples[accept_mask])\n",
    "                        filtered_CL_samples.extend(CL_samples[accept_mask])\n",
    "\n",
    "                        if  len(filtered_CL_samples) > desired_samples:\n",
    "                            break\n",
    "                        \n",
    "                        attempts += 1\n",
    "                        tqdm_iterable.set_postfix({'Samples':len(filtered_CL_samples)})\n",
    "\n",
    "                    # Truncate to desired number of samples\n",
    "                    filtered_cp_samples = filtered_cp_samples[:desired_samples]\n",
    "                    filtered_cl_samples = filtered_cl_samples[:desired_samples]\n",
    "                    filtered_CL_samples = filtered_CL_samples[:desired_samples]\n",
    "                    \n",
    "                    # Make into np array\n",
    "                    filtered_cp_samples = np.vstack(filtered_cp_samples)\n",
    "                    filtered_cl_samples = np.vstack(filtered_cl_samples)\n",
    "                    filtered_CL_samples = np.vstack(filtered_CL_samples).flatten()\n",
    "                    \n",
    "                    print(f\"Accepted {len(filtered_CL_samples)} samples after {attempts} attempts.\")\n",
    "                    plt.hist(filtered_CL_samples, bins=20)\n",
    "                    conditioned_cp_posterior = torch.distributions.multivariate_normal.MultivariateNormal(torch.mean(torch.from_numpy(filtered_cp_samples), dim=0).flatten(), \n",
    "                                                                                                          torch.cov(torch.from_numpy(filtered_cp_samples).T) + torch.eye(filtered_cp_samples.shape[1])*1e-6)\n",
    "                    return conditioned_cp_posterior, filtered_CL_samples\n",
    "        \n",
    "        # If user wants to save \n",
    "        if save_to is not None:\n",
    "            # self.__write_to_mesh(test_wing=test_wing, case_solution=output_distr, output_filename=save_to)\n",
    "            self.__save_to_solution_file(test_wing=test_wing, output_filename=save_to)\n",
    "        return output_distr\n",
    "    \n",
    "    \"\"\"\n",
    "    Save predictions out to a reference mesh file \n",
    "    .dat file exported from tecplot \n",
    "    \"\"\"\n",
    "    def __save_to_solution_file(self, test_wing, output_filename):\n",
    "            write_file = []\n",
    "            test_wing.mesh['header']\n",
    "            delimiter = '\\n'\n",
    "            write_file.append(delimiter.join(test_wing.mesh['header']))\n",
    "            for i in range(len(test_wing.mesh['zones_data'])):\n",
    "                # append the zone header \n",
    "                write_file.append(delimiter.join(test_wing.mesh['zones_header'][i])) \n",
    "                # Convert zone data into string \n",
    "                data_lines = []\n",
    "                for row in test_wing.mesh['zones_data'][i]:\n",
    "                    formatted = ['{: .9E}'.format(row[0])]  # First value with leading space\n",
    "                    formatted += ['{:.9E}'.format(val) for val in row[1:]]  # Rest of the row\n",
    "                    line = ' '.join(formatted)\n",
    "                    data_lines.append(line)\n",
    "                data_str = '\\n'.join(data_lines)\n",
    "                write_file.append(data_str)\n",
    "                write_file.append(delimiter.join(test_wing.mesh['zones_connectivity'][i])) \n",
    "            # Join all together \n",
    "            write_file = delimiter.join(write_file)\n",
    "            \n",
    "            with open(output_filename, \"w\") as f:\n",
    "                f.write(write_file)\n",
    "                \n",
    "    def __write_to_mesh(self, test_wing, case_solution:gpytorch.distributions.MultivariateNormal, output_filename:str=None):\n",
    "        # Default file name is the original file name with lwm appended\n",
    "        if output_filename is None:\n",
    "            output_filename = test_wing.mesh.file_directory[:-3] + 'lwm.dat'\n",
    "            \n",
    "        # Read in node information\n",
    "        wing_nodes = test_wing.mesh['nodes'][0]\n",
    "        all_nodes = test_wing.mesh['nodes'][1]\n",
    "        \n",
    "        # Mesh file block 1 - header w textual information\n",
    "        title_str = f'TITLE:     = \"{test_wing.case_name}\"'\n",
    "        variable_str = 'VARIABLES = \"X\"\\n\"Y\"\\n\"Z\"\\n\"mean C_p\"\\n\"Confidence region\"'\n",
    "        zone_str = 'ZONE T=\"wing\"'\n",
    "        id_str = 'STRANDID=1, SOLUTIONTIME=2'\n",
    "        mesh_str = f'Nodes={str(np.unique(wing_nodes[:,1:]).shape[0])}, Elements={str(test_wing.mesh[\"N_marker_elems\"])}, ZONETYPE={test_wing.mesh[\"type_str\"]}'\n",
    "        pack_str = 'DATAPACKING=POINT'\n",
    "        DT_str = 'DT=(SINGLE SINGLE SINGLE SINGLE, SINGLE)'\n",
    "        block1 = [title_str, variable_str, zone_str, id_str, mesh_str, pack_str, DT_str]\n",
    "\n",
    "        # block 2 - coordinates info\n",
    "            # get unique node numbers\n",
    "        wing_node_indices = np.sort(np.unique(wing_nodes[:,1:])) # indices of wing nodes\n",
    "        wing_node_new_indices = np.arange(1, np.unique(wing_nodes[:,1:]).shape[0]+1)\n",
    "        mask = np.isin(all_nodes[:, -1].astype(int), wing_node_indices)\n",
    "        wing_node_coordinates = all_nodes[mask][:, :3] # coordinates of wing nodes \n",
    "            \n",
    "        case_solution_mean = case_solution.mean[:,None].cpu().detach().numpy()\n",
    "        case_solution_2sig = 2*np.sqrt(np.diag(case_solution.covariance_matrix.cpu().detach().numpy()))[:, None]\n",
    "        block2 = np.hstack((wing_node_coordinates, case_solution_mean, case_solution_2sig)).astype(float) \n",
    "\n",
    "        # block 3 - nodes info\n",
    "        block3 = np.zeros_like(wing_nodes[:, 1:])\n",
    "        for j in range(1, wing_nodes.shape[1]):\n",
    "            for i in range(wing_nodes.shape[0]):\n",
    "                new_val = wing_node_new_indices[np.argwhere(wing_node_indices==wing_nodes[i, j])[0][0]]\n",
    "                block3[i, j-1] = new_val\n",
    "                \n",
    "        # save file \n",
    "        with open(output_filename, \"w\") as f:\n",
    "            # Write each string on a new line\n",
    "            for line in block1:\n",
    "                f.write(line + \"\\n\")\n",
    "            \n",
    "            # Write arrays back to back\n",
    "            np.savetxt(f, block2, fmt='%.6f', delimiter=' ')\n",
    "            np.savetxt(f, block3, fmt='%d', delimiter=' ')\n",
    "            print_with_tag(f'Prediction saved as {output_filename}', 'LWM')\n",
    "    \n",
    "    def __plot_directly(self, test_wing, case_solution:gpytorch.distributions.MultivariateNormal, output_filename:str=None):\n",
    "        1\n",
    "        \n",
    "## Main DKL Model\n",
    "class DKL_model(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, # GP parameters\n",
    "                     feature_extractor, # NN parameters \n",
    "                     lam_priors:lam_prior=None, # LAM-prior parameters\n",
    "                     verbose:bool=False):\n",
    "            super(DKL_model, self).__init__(train_x, train_y, likelihood)\n",
    "            self.feature_extractor = feature_extractor\n",
    "            self.lam_priors = lam_priors\n",
    "            self.spanwise_dim_idx = 58 # Index in the training data for the spanwise location, eta\n",
    "            self.verbose = verbose\n",
    "            self.train_x = train_x \n",
    "            print_with_tag('Intializing the model', 'LOG') if verbose else None\n",
    "            # Intialize the model with or without the LAM-prior depending on user input\n",
    "            if self.lam_priors is not None:\n",
    "                print_with_tag('The model is using a LAM prior', 'LOG') if verbose else None\n",
    "                self.mean_module = self.lam_priors.mean_module # LAM-prior prior mean \n",
    "                self.prior_cov = self.lam_priors.covar_module # LAM-prior prior covariance, this is used in the forward method \n",
    "            else: \n",
    "                print_with_tag('The model is using a constant mean prior', 'LOG') if verbose else None\n",
    "                self.mean_module = gpytorch.means.ConstantMean()\n",
    "                \n",
    "            # Set up covariance module \n",
    "            self.covar_module = gpytorch.kernels.MaternKernel(nu=5/2, ard_num_dims=self.feature_extractor.nn_dims[-1], \n",
    "                                                                    lengthscale_prior=gpytorch.priors.NormalPrior(0.5, 1.0), \n",
    "                                                                    active_dims = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ]) * SVH_Matern_2var_1d(rates=[15], active_dims = [14])# Covariance module  \n",
    "\n",
    "            # Set up other DKL things  \n",
    "            self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1.0, 1.0) # scale the feature extractor outputs \n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor) \n",
    "            projected_x = self.feature_extractor(torch.hstack((x[:,:self.spanwise_dim_idx], x[:,self.spanwise_dim_idx+1:-1])))  # remove the spanwise loc -> feed into feature extractor\n",
    "            projected_x = self.scale_to_bounds(projected_x)  # scale to nice values \n",
    "            projected_x = torch.hstack((projected_x, x[:, self.spanwise_dim_idx:self.spanwise_dim_idx+1])) # add back in the spanwise dimension\n",
    "            \n",
    "            # Run latent variables thru the GP model \n",
    "            mean_x = self.mean_module(x)\n",
    "            if self.lam_priors is not None:\n",
    "                covar_x = self.covar_module(projected_x) + self.prior_cov(x)\n",
    "            elif self.lam_priors is None: \n",
    "                covar_x = self.covar_module(projected_x)\n",
    "            else: \n",
    "                raise ValueError('Invalid prior type')\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        \n",
    "        \n",
    "## Feature Extractor \n",
    "class nn_feature_extractor(torch.nn.Sequential):\n",
    "    def __init__(self, train_x, nn_dims):\n",
    "        self.nn_dims = nn_dims\n",
    "        super(nn_feature_extractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(train_x.shape[-1]-1-1, self.nn_dims[0])) # __\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('dropout1', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear2', torch.nn.Linear(self.nn_dims[0], self.nn_dims[1]))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('dropout2', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear3', torch.nn.Linear(self.nn_dims[1], self.nn_dims[2]))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('dropout3', torch.nn.Dropout(0.2))\n",
    "        self.add_module('linear4', torch.nn.Linear(self.nn_dims[2], self.nn_dims[3]))\n",
    "        if len(self.nn_dims) > 4:\n",
    "            self.add_module('relu4', torch.nn.ReLU())\n",
    "            self.add_module('dropout4', torch.nn.Dropout(0.2))\n",
    "            self.add_module('linear5', torch.nn.Linear(self.nn_dims[3], self.nn_dims[4]))\n",
    "            self.add_module('relu5', torch.nn.ReLU())\n",
    "            self.add_module('dropout6', torch.nn.Dropout(0.2))\n",
    "            self.add_module('linear6', torch.nn.Linear(self.nn_dims[4], self.nn_dims[5]))\n",
    "            \n",
    "class input_wing():\n",
    "    #We want the following from the CSV = ['xc', 'yb', 'surf', 'airfoil', 'alpha', 'M', 'Re', 'chord',' taper ratio', 'span', 'le sweep', 'cp', 'source']\n",
    "    def __init__(self, airfoil:lam_adapt.input_data, chord_length:float, \n",
    "                 span_length:float, sweep_angle:float, taper_ratio:float, reynolds_number:float, \n",
    "                 mach_number:float, angle_of_attack:float, case_num:int, output_device:torch.device='cpu', verbose:bool=False,\n",
    "                 source_file:str=None, coordinate_marker:str=None, case_name:str='WING',\n",
    "                 span_resolution=20):\n",
    "        # CASE NUM NEEDS TO BE REPLACED EVENTUALLY\n",
    "        self.case_num = case_num\n",
    "        self.case_name = case_name \n",
    "        self.verbose = verbose \n",
    "        self.output_device = output_device \n",
    "        \n",
    "        self.airfoil = airfoil\n",
    "        self.chord_dimensional = chord_length # dimensional chord length, root chord assumed for now \n",
    "        self.span_dimensional = span_length # dimensional span length\n",
    "        self.le_sweep = sweep_angle # leading edge sweep angle in degrees, \\Lambda\n",
    "        self.taper_ratio = taper_ratio # taper ratio, \\lambda\n",
    "        self.reynolds = reynolds_number # Re (not used)\n",
    "        self.mach = mach_number # Freestream mach number \n",
    "        self.alph = angle_of_attack  # angle of attack (degrees)\n",
    "        self.alph_rad = np.deg2rad(angle_of_attack) # angle of attack (radians)\n",
    "        \n",
    "        # Derived quantities \n",
    "        self.semispan_ratio = self.span_dimensional / self.chord_dimensional # semi-span to c_root ratio (s/c_root)\n",
    "        self.xhat = None # xhat and zhat, should be filled out when running assemble_tensor() function\n",
    "        self.zhat = None # \n",
    "        self.eta = None # \n",
    "        self.coordinates = None # this ?  \n",
    "        self.case_title = None\n",
    "        self.input_tensor = None\n",
    "        self.mesh = {\n",
    "                'file_directory': None,\n",
    "                'coordinate_marker': None,\n",
    "                'header': None,\n",
    "                'zones_header': [],\n",
    "                'zones_data': [],  \n",
    "                'zones_connectivity': [],\n",
    "                'type_str': None\n",
    "            } \n",
    "        self.coordinate_marker = coordinate_marker\n",
    "        \n",
    "        # tensor quantity \n",
    "        self.span_resolution = span_resolution # Number of unique spanwise locations, either an int or an array of pre-determine locations\n",
    "        self.chord_resolution = 200 # per surface \n",
    "        # Use mesh source file if given, make default tensor if not \n",
    "        if source_file is not None: \n",
    "            assert coordinate_marker is not None, \"A list of marker string that defines the coordinates is required\"\n",
    "            self.mesh['file_directory'] = source_file\n",
    "            self.mesh['coordinate_marker'] = coordinate_marker\n",
    "            self.input_type = 'mesh' \n",
    "        else: \n",
    "            self.input_type = 'default' \n",
    "            \n",
    "    def assemble_tensor(self):\n",
    "        if self.input_type == 'default':\n",
    "            out_tensor = []\n",
    "            # Initialize chordwise coordinates \n",
    "            xhat_init = np.hstack((np.linspace(-1.0, 1.0, self.chord_resolution), np.linspace(-1.0, 1.0, self.chord_resolution)))\n",
    "            zhat_init = np.ones_like(xhat_init)\n",
    "            zhat_init[:self.chord_resolution] = np.sin(np.arccos(xhat_init[:self.chord_resolution]))\n",
    "            zhat_init[0] = 0.0\n",
    "            zhat_init[self.chord_resolution:] = -np.sin(np.arccos(xhat_init[self.chord_resolution:]))\n",
    "            zhat_init[self.chord_resolution] = 0.0\n",
    "            # Set up eta \n",
    "            if isinstance(self.span_resolution, int): # if int, do uniform spacing\n",
    "                self.eta = np.repeat(np.linspace(0.0, 1.0, self.span_resolution), self.chord_resolution*2) \n",
    "            elif isinstance(self.span_resolution, np.ndarray): \n",
    "                self.eta = np.repeat(self.span_resolution, self.chord_resolution*2)  \n",
    "            else: \n",
    "                raise ValueError(\"Invalid spanwise resolution\")\n",
    "            # Tile out full coordinates\n",
    "            xhat = np.tile(xhat_init, self.span_resolution) if isinstance(self.span_resolution, int) else np.tile(xhat_init, self.span_resolution.shape[0])\n",
    "            zhat = np.tile(zhat_init, self.span_resolution) if isinstance(self.span_resolution, int) else np.tile(zhat_init, self.span_resolution.shape[0]) \n",
    "            # Other stuff\n",
    "            alpha_array = np.ones_like(xhat) * self.alph_rad\n",
    "            mach_array = np.ones_like(xhat) * self.mach\n",
    "            taper_array = np.ones_like(xhat) * self.taper_ratio\n",
    "            le_sweep_array = np.ones_like(xhat) * self.le_sweep\n",
    "            semispan_array = np.ones_like(xhat) * self.semispan_ratio\n",
    "            case_array = np.ones_like(xhat) * self.case_num\n",
    "            self.xhat = xhat \n",
    "            self.zhat = zhat \n",
    "            # Get airfoil coordinates\n",
    "            out_tensor_pt1 = torch.from_numpy(np.tile(self.airfoil.retrieve_airfoil_input(true_values=True), (xhat.shape[0], 1))) \n",
    "            print('Reminder: temp fix remove')\n",
    "            out_tensor_pt1 = torch.hstack((torch.flip(out_tensor_pt1[:, :28], dims=[1]), out_tensor_pt1[:, 28:]))# FIX \n",
    "            # Assemble tensor  \n",
    "            out_tensor_pt2 = torch.from_numpy(np.hstack((self.xhat[:, None], self.zhat[:, None], self.eta[:, None], \n",
    "                                alpha_array[:, None], mach_array[:, None], taper_array[:, None], \n",
    "                                le_sweep_array[:, None], semispan_array[:, None], case_array[:, None])))\n",
    "            if len(out_tensor) == 0: # only append if this is the first time assemble_tensor has been run\n",
    "                out_tensor.append(torch.hstack((out_tensor_pt1, out_tensor_pt2)).to(self.output_device) )\n",
    "            self.input_tensor = out_tensor\n",
    "              \n",
    "        elif self.input_type == 'mesh':\n",
    "            out_tensor = self._assemble_from_solution()\n",
    "            print_with_tag('Input tensor assembled from source mesh file', 'WING') if self.verbose else None\n",
    "        else: \n",
    "            print('Implement more here ')\n",
    "        return out_tensor\n",
    "    \n",
    "    \n",
    "    def _assemble_from_solution(self):\n",
    "        out_tensor = []\n",
    "        assert self.mesh['file_directory'] is not None, \"You must provide a source solutions file\"\n",
    "        file_path = self.mesh['file_directory']\n",
    "        coordinate_names = [f'\"{self.coordinate_marker[0]}\"', f'\"{self.coordinate_marker[1]}\"',  f'\"{self.coordinate_marker[2]}\"']\n",
    "\n",
    "        file_title = \"dummy file\"  # Replace this \n",
    "\n",
    "        output_header = [] \n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            # Figure out how many zones there are \n",
    "            zone_idx = [match.start() for match in re.finditer('ZONE T', file_content)]\n",
    "            num_zones = len(zone_idx)\n",
    "            zone_idx = zone_idx + [len(file_content)]\n",
    "            \n",
    "            # Extract Meta-information from file header \n",
    "            print('[LOG] Reading file header...')\n",
    "            current_line = file_content\n",
    "            line_start_idx = 0 # index where the line starts \n",
    "            entry_str = ''\n",
    "            while entry_str != 'ZONE T': # repeat until zones \n",
    "                # Isolate line \n",
    "                current_line = file_content[line_start_idx:]\n",
    "                line_end_idx = [m.start() for m in re.finditer(r'\\n ?[A-Z]', current_line)][0] #current_line.find('\\n') # index where the line ends \n",
    "                current_line = current_line[:line_end_idx] # current line \n",
    "\n",
    "                # Identify what its about  \n",
    "                if '=' in current_line:\n",
    "                    entry_str = current_line.split('=')[0].strip() \n",
    "                    \n",
    "                # Perform relevant action depending on line \n",
    "                match entry_str: \n",
    "                    case \"TITLE\": # Assign new title \n",
    "                        new_entry  = \"TITLE = \" + file_title\n",
    "                        output_header.append(new_entry) \n",
    "                    case \"VARIABLES\": # Identify variable names \n",
    "                        list_variables = current_line.split('=')[1].split('\\n')\n",
    "                        list_variables = [i.strip() for i in list_variables]\n",
    "                        coordinates_idx = [list_variables.index(j) for j in coordinate_names]\n",
    "\n",
    "                        new_entry = 'VARIABLES = \"x/c\"\\n\"y/b\"\\n\"z/c\"\\n\"Pressure coefficient\"\\n\"Confidence interval\"'\n",
    "                        output_header.append(new_entry)  \n",
    "                \n",
    "                # Move onto next line \n",
    "                line_start_idx += line_end_idx + 1 # +2 due to \\n\n",
    "            \n",
    "            # Iterate through zones and calculate the model predicted values \n",
    "            for i in range(num_zones): # [:1]\n",
    "                print(f'[LOG] Reading Zone {i+1}...')\n",
    "                zone_content = file_content[zone_idx[i]:zone_idx[i+1]] # The main \"zone data\"\n",
    "                zone_header = []\n",
    "                current_line = zone_content\n",
    "                line_start_idx = 0 # index where the line starts \n",
    "                line_end_idx = 0\n",
    "                current_line = zone_content[line_start_idx:]\n",
    "                entry_str = ''\n",
    "                while len([m.start() for m in re.finditer(r'\\n ?[A-Z]', current_line)]) != 0: # repeat until the data matrix \n",
    "                    # Isolate line  \n",
    "                    line_end_idx = [m.start() for m in re.finditer(r'\\n ?[A-Z]', current_line)][0] # index where the line ends \n",
    "                    current_line = current_line[:line_end_idx] # current line \n",
    "\n",
    "                    # Identify what its about  \n",
    "                    if '=' in current_line:\n",
    "                        entry_str = current_line.split('=')[0].strip() \n",
    "                    # Perform relevant action depending on line \n",
    "                    match entry_str: \n",
    "                        case \"ZONE T\": \n",
    "                            # Add the new_entry\n",
    "                            zone_header.append(current_line) \n",
    "                        case \"STRANDID\": # STRANDID=1, SOLUTIONTIME=0\n",
    "                            # Nothing needs to change here, append the same thing \n",
    "                            zone_header.append(current_line) \n",
    "                        case \"Nodes\": # Nodes=1579, Elements=919, ZONETYPE=FETriangle\n",
    "                            split_line = current_line.split(',')\n",
    "                            nodes, elements, zonetype = split_line[0], split_line[1], split_line[2] \n",
    "                            num_nodes = int(nodes.split('=')[-1]) # number of nodes in mesh ## \n",
    "                            num_elems = int(elements.split('=')[-1]) # number of elements in mesh ## \n",
    "                            zonetype = zonetype.split('=')[-1]  # zone type  \n",
    "                            \n",
    "                            # Nothing needs to change here, append the same thing \n",
    "                            zone_header.append(current_line) \n",
    "                        case \"DATAPACKING\": # DATAPACKING=POINT\n",
    "                            # Nothing needs to change here, append the same thing \n",
    "                            zone_header.append(current_line) \n",
    "                        case \"AUXDATA Time\": \n",
    "                            # Nothing needs to change here, append the same thing \n",
    "                            zone_header.append(current_line) \n",
    "                    \n",
    "                    # Move onto next line \n",
    "                    line_start_idx += line_end_idx + 1 # \n",
    "                    current_line = zone_content[line_start_idx:]\n",
    "                    \n",
    "                # Add DT entry       \n",
    "                new_entry = \" DT=(SINGLE SINGLE SINGLE SINGLE SINGLE )\" # requires 5, x, y, z, Cp, 2sig range\n",
    "                zone_header.append(new_entry) \n",
    "                \n",
    "                mesh_information = zone_content[zone_content.find(')\\n')+2:] \n",
    "                output_locations = mesh_information.split('\\n')[:num_nodes] # Locations at which predictions will be generated\n",
    "                output_connectivity = mesh_information.split('\\n')[num_nodes:-1] # Information regarding connectivity of nodes \n",
    "\n",
    "                # Convert output locations to LWM input \n",
    "                output_locations_array = np.array([[float(num) for num in line.split()] for line in output_locations])\n",
    "                wing_node_xc = output_locations_array[:, coordinates_idx[0]]\n",
    "                wing_node_yb = output_locations_array[:, coordinates_idx[1]]\n",
    "                wing_node_zc = output_locations_array[:, coordinates_idx[2]]\n",
    "                \n",
    "                xc = wing_node_xc / self.chord_dimensional # This needs to be converted to hats \n",
    "                xc[xc>1.0] = 1.0 # cut off\n",
    "                zc = wing_node_zc / self.chord_dimensional # This needs to be converted to hats\n",
    "                xhat = xc*2 - 1 # transform to xhat \n",
    "                zhat = np.zeros_like(xhat)\n",
    "                if len(zhat[zc>0.0]) > 0:\n",
    "                    zhat[zc>0.0] = np.sin(np.arccos(xhat[zc>0.0])) # transform to zhat for upper\n",
    "                if len(zhat[zc<=0.0]) > 0:\n",
    "                    zhat[zc<=0.0] = -np.sin(np.arccos(xhat[zc<=0.0])) # transform to zhat for lower\n",
    "                # self.zhat = zhat \n",
    "                # self.xhat = xhat \n",
    "                self.eta = wing_node_yb / 3.35#self.span_dimensional # get eta, non-dimensional spanwise\n",
    "                # generate other parts of the input tensor\n",
    "                alpha_array = np.ones_like(xhat) * self.alph_rad\n",
    "                mach_array = np.ones_like(xhat) * self.mach\n",
    "                taper_array = np.ones_like(xhat) * self.taper_ratio\n",
    "                le_sweep_array = np.ones_like(xhat) * self.le_sweep\n",
    "                semispan_array = np.ones_like(xhat) * self.semispan_ratio\n",
    "                case_array = np.ones_like(xhat) * self.case_num\n",
    "                self.xhat = xhat \n",
    "                self.zhat = zhat\n",
    "                # Get airfoil coordinates\n",
    "                out_tensor_pt1 = torch.from_numpy(np.tile(self.airfoil.retrieve_airfoil_input(true_values=True), (xhat.shape[0], 1)))  \n",
    "                print('temp fix')\n",
    "                out_tensor_pt1 = torch.hstack((torch.flip(out_tensor_pt1[:, :28], dims=[1]), out_tensor_pt1[:, 28:]))# FIX f\n",
    "                # Assemble tensor  \n",
    "                out_tensor_pt2 = torch.from_numpy(np.hstack((self.xhat[:, None], self.zhat[:, None], self.eta[:, None], \n",
    "                                    alpha_array[:, None], mach_array[:, None], taper_array[:, None], \n",
    "                                    le_sweep_array[:, None], semispan_array[:, None], case_array[:, None])))\n",
    "                out_tensor.append(torch.hstack((out_tensor_pt1, out_tensor_pt2)).to(self.output_device))\n",
    "                # Store useful information to dict \n",
    "                self.mesh['zones_connectivity'].append(output_connectivity)\n",
    "                self.mesh['zones_header'].append(zone_header)\n",
    "                self.mesh['zones_data'].append(np.hstack((wing_node_xc[:, None], wing_node_yb[:, None], wing_node_zc[:, None], np.zeros_like(wing_node_zc[:, None]), np.zeros_like(wing_node_zc[:, None])))) \n",
    "                # Cp and 2sig are dummy data that must be replaced during predictions\n",
    "            # Store more useful information to dict  \n",
    "            self.mesh['header'] = output_header\n",
    "            \n",
    "            \n",
    "            self.input_tensor = out_tensor \n",
    "            self.input_type = 'mesh'\n",
    "            return out_tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Computing cache for training data mean.\n",
      "[LOG] Computing cache for training data covariance matrix.\n",
      "[LOG] Intializing the model\n",
      "[LOG] The model is using a LAM prior\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "# initialize LAM-priors\n",
    "lam_priors = lam_prior(priors=[mean_list, cov_list, xhat_locs], output_device=output_device, case_guides=all_cases_unique, verbose=True) \n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(torch.ones_like(train_y)*0.01 * scaler_scale**2, learn_additional_noise=False, noise_prior=gpytorch.priors.NormalPrior(0.1*scaler_scale, 0.01)) \n",
    "feature_extractor = nn_feature_extractor(train_x, nn_dims = [1000, 1000, 1000, 500, 50, 14]) \n",
    "model = DKL_model(train_x, train_y, likelihood, \n",
    "               feature_extractor=feature_extractor, \n",
    "               lam_priors=lam_priors,  # \n",
    "               verbose=True) \n",
    "\n",
    "# Define Optimizer  \n",
    "lr = 1e-3 \n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()},], lr=lr)  \n",
    "scheduler = StepLR(optimizer, step_size=2000, gamma=0.1)\n",
    "\n",
    "# Create LWM instance \n",
    "lwm = large_wing_model(model, likelihood, optimizer, scheduler, output_device=output_device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6487d2e4ed420686aef66b75fdf42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lwm.train_model(10, save_name='testrun_', save_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create an instance of the input_wing class\n",
    "chordwise_var_name = 'CoordinateX'\n",
    "spanwise_var_name = 'CoordinateY'\n",
    "chordnormal_var_name = 'CoordinateZ'\n",
    "\n",
    "test_airfoil = lam_adapt.input_data('NACA 0015', 4.0, 0.2,  output_device='cpu', model_version='v2', ) # the AoA and M does not matter here\n",
    "pred_wing = input_wing(test_airfoil, chord_length=1, span_length=3.3, sweep_angle=0, taper_ratio=1, \n",
    "                       reynolds_number=_, mach_number=0.2, angle_of_attack=4.0, case_num=target_case, output_device=output_device,\n",
    "                       source_file='example_mesh.dat', coordinate_marker=[chordwise_var_name, spanwise_var_name, chordnormal_var_name],\n",
    "                       verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Reading file header...\n",
      "[LOG] Reading Zone 1...\n",
      "temp fix\n",
      "[LOG] Reading Zone 2...\n",
      "temp fix\n",
      "[WING] Input tensor assembled from source mesh file\n",
      "[LWM] Generating prediction...\n",
      "[LWM] Loading weight: ./weights/weights_1\n",
      "[LWM] Loading weight: ./weights/weights_2\n",
      "runtime: 1.9324729442596436\n",
      "[LWM] Generating prediction...\n",
      "[LWM] Loading weight: ./weights/weights_1\n",
      "[LWM] Loading weight: ./weights/weights_2\n",
      "runtime: 22.930493593215942\n"
     ]
    }
   ],
   "source": [
    "# Generate prediction\n",
    "prediction = lwm.predict(pred_wing, save_to=None, \n",
    "                         target_weights=['./weights/weights_1',\n",
    "                                         './weights/weights_2'], )    \n",
    "# condition = {'cl': [0.353, 0.001], 'num_samples':1000} add this in for posterior conditioning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
